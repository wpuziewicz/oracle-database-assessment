{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Database Migration Advisor Moving Oracle Workloads to Google Cloud Get the recommended Google Cloud configuration your current Oracle environments. Facts based approach to sizing that leverages metadata from your environment. Support all flavors of Oracle from 11g to 21c - Exadata, RDS, and OCI workloads included Leverages AWR data (requires tuning and diagnostics pack for Oracle) or statspack for sizing. Quick Start Grab the collection scripts from the latest release here . Instructions for execution are included in the README bundled with the collection scripts.","title":"Home"},{"location":"#database-migration-advisor","text":"","title":"Database Migration Advisor"},{"location":"#moving-oracle-workloads-to-google-cloud","text":"Get the recommended Google Cloud configuration your current Oracle environments. Facts based approach to sizing that leverages metadata from your environment. Support all flavors of Oracle from 11g to 21c - Exadata, RDS, and OCI workloads included Leverages AWR data (requires tuning and diagnostics pack for Oracle) or statspack for sizing.","title":"Moving Oracle Workloads to Google Cloud"},{"location":"#quick-start","text":"Grab the collection scripts from the latest release here . Instructions for execution are included in the README bundled with the collection scripts.","title":"Quick Start"},{"location":"code-of-conduct/","text":"Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. This Code of Conduct also applies outside the project spaces when the Project Steward has a reasonable belief that an individual's behavior may have a negative impact on the project or its community. Conflict Resolution We do not believe that all conflict is bad; healthy debate and disagreement often yield positive results. However, it is never okay to be disrespectful or to engage in behavior that violates the project\u2019s code of conduct. If you see someone violating the code of conduct, you are encouraged to address the behavior directly with those involved. Many issues can be resolved quickly and easily, and this gives people more control over the outcome of their dispute. If you are unable to resolve the matter for any reason, or if the behavior is threatening or harassing, report it. We are dedicated to providing an environment where participants feel welcome and safe. Reports should be directed to [PROJECT STEWARD NAME(s) AND EMAIL(s)] , the Project Steward(s) for [PROJECT NAME] . It is the Project Steward\u2019s duty to receive and address reported violations of the code of conduct. They will then work with a committee consisting of representatives from the Open Source Programs Office and the Google Open Source Strategy team. If for any reason you are uncomfortable reaching out to the Project Steward, please email opensource@google.com. We will investigate every complaint, but you may not receive a direct response. We will use our discretion in determining when and how to follow up on reported incidents, which may range from not taking action to permanent expulsion from the project and project-sponsored spaces. We will notify the accused of the report and provide them an opportunity to discuss it before any action is taken. The identity of the reporter will be omitted from the details of the report supplied to the accused. In potentially harmful situations, such as ongoing harassment or threats to anyone's safety, we may take action without notice. Attribution This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html","title":"Code Of Conduct"},{"location":"code-of-conduct/#code-of-conduct","text":"","title":"Code of Conduct"},{"location":"code-of-conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"code-of-conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"code-of-conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"code-of-conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. This Code of Conduct also applies outside the project spaces when the Project Steward has a reasonable belief that an individual's behavior may have a negative impact on the project or its community.","title":"Scope"},{"location":"code-of-conduct/#conflict-resolution","text":"We do not believe that all conflict is bad; healthy debate and disagreement often yield positive results. However, it is never okay to be disrespectful or to engage in behavior that violates the project\u2019s code of conduct. If you see someone violating the code of conduct, you are encouraged to address the behavior directly with those involved. Many issues can be resolved quickly and easily, and this gives people more control over the outcome of their dispute. If you are unable to resolve the matter for any reason, or if the behavior is threatening or harassing, report it. We are dedicated to providing an environment where participants feel welcome and safe. Reports should be directed to [PROJECT STEWARD NAME(s) AND EMAIL(s)] , the Project Steward(s) for [PROJECT NAME] . It is the Project Steward\u2019s duty to receive and address reported violations of the code of conduct. They will then work with a committee consisting of representatives from the Open Source Programs Office and the Google Open Source Strategy team. If for any reason you are uncomfortable reaching out to the Project Steward, please email opensource@google.com. We will investigate every complaint, but you may not receive a direct response. We will use our discretion in determining when and how to follow up on reported incidents, which may range from not taking action to permanent expulsion from the project and project-sponsored spaces. We will notify the accused of the report and provide them an opportunity to discuss it before any action is taken. The identity of the reporter will be omitted from the details of the report supplied to the accused. In potentially harmful situations, such as ongoing harassment or threats to anyone's safety, we may take action without notice.","title":"Conflict Resolution"},{"location":"code-of-conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html","title":"Attribution"},{"location":"contributing/","text":"How to Contribute We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow. Contributor License Agreement Contributions to this project must be accompanied by a Contributor License Agreement. You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://cla.developers.google.com/ to see your current agreements on file or to sign a new one. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again. Code Reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests. Community Guidelines This project follows Google's Open Source Community Guidelines .","title":"Contributing"},{"location":"contributing/#how-to-contribute","text":"We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow.","title":"How to Contribute"},{"location":"contributing/#contributor-license-agreement","text":"Contributions to this project must be accompanied by a Contributor License Agreement. You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://cla.developers.google.com/ to see your current agreements on file or to sign a new one. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again.","title":"Contributor License Agreement"},{"location":"contributing/#code-reviews","text":"All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.","title":"Code Reviews"},{"location":"contributing/#community-guidelines","text":"This project follows Google's Open Source Community Guidelines .","title":"Community Guidelines"},{"location":"developer_guide/commands/","text":"Commands make install - Creates a new virtual environment for development. make clean - Remove all build, testing, and static documentation files. make format-source - Formats the source code with isort and black. make upgrade-dependencies - Upgrades all dependencies to the latest compatible version. make build-collection - Build a folder containing a set of the latest database collection scripts. make package-collection - Generate an archive of the database collection scripts. To be used after executing build make gen-docs - Generate HTML documentation. make docs - Generate HTML documentation and serve it to the browser. make pre-release increment={major/minor/patch} - Bump the version and create a release tag. Should only be run from the main branch. Passes the increment value to bump2version to create a new version number dynamically. The new version number will be added to __version__.py and pyproject.toml and a new commit will be logged. The tag will be created from the new commit.","title":"Commands"},{"location":"developer_guide/commands/#commands","text":"make install - Creates a new virtual environment for development. make clean - Remove all build, testing, and static documentation files. make format-source - Formats the source code with isort and black. make upgrade-dependencies - Upgrades all dependencies to the latest compatible version. make build-collection - Build a folder containing a set of the latest database collection scripts. make package-collection - Generate an archive of the database collection scripts. To be used after executing build make gen-docs - Generate HTML documentation. make docs - Generate HTML documentation and serve it to the browser. make pre-release increment={major/minor/patch} - Bump the version and create a release tag. Should only be run from the main branch. Passes the increment value to bump2version to create a new version number dynamically. The new version number will be added to __version__.py and pyproject.toml and a new commit will be logged. The tag will be created from the new commit.","title":"Commands"},{"location":"developer_guide/developer_setup/","text":"Developer Setup To begin local development, clone the GoogleCloudPlatform/oracle-database-assessment repository and use one of the following methods to build it. All commands should be executed from inside of the project home folder. Configure environment make install","title":"Developer Setup"},{"location":"developer_guide/developer_setup/#developer-setup","text":"To begin local development, clone the GoogleCloudPlatform/oracle-database-assessment repository and use one of the following methods to build it. All commands should be executed from inside of the project home folder.","title":"Developer Setup"},{"location":"developer_guide/developer_setup/#configure-environment","text":"make install","title":"Configure environment"},{"location":"developer_guide/releases/","text":"Releases A release should consist of the following two steps from a tested, linted, and up to date copy of the main branch: make pre-release increment={major/minor/patch} - Commit the version number bump and create a new tag locally. The version number follows semantic versioning standards (major.minor.patch) and the tag is the version number prepended with a 'v'. git push --follow-tags - Update the main branch with only the changes from the version bump. Publish the new tag and kick off the release workflow.","title":"Releases"},{"location":"developer_guide/releases/#releases","text":"A release should consist of the following two steps from a tested, linted, and up to date copy of the main branch: make pre-release increment={major/minor/patch} - Commit the version number bump and create a new tag locally. The version number follows semantic versioning standards (major.minor.patch) and the tag is the version number prepended with a 'v'. git push --follow-tags - Update the main branch with only the changes from the version bump. Publish the new tag and kick off the release workflow.","title":"Releases"},{"location":"developer_guide/workflows/","text":"Workflows Test Run the tests on every push/pull request to the _main branch. Writes a coverage report using pytest-cov and uploads it to codecov.io. Tests run against python versions 3.8 and 3.9. Optional manual trigger in the github actions tab. Lint Run the linting tools on every push/pull request to the _main branch. Includes pre-commit hooks, black, isort, flake8, pylint, and mypy. Optional manual trigger in the github actions tab. Release Build a wheel distribution, build a docker image, create a github release, and publish to PyPI and Docker Hub whenever a new tag is created. Linting and testing steps must pass before the release steps can begin. Documentation is automatically published to the gh-pages branch and hosted on github pages. All github release tags, docker image tags, and PyPI version numbers are in agreement with one another and follow semantic versioning standards. Build and Publish Docs Build the documentation, publish to the gh-pages branch, and release to github pages. Runs only on a manual trigger in the github actions tab. Build and Publish Docker Image Build the docker image, tag it with the branch name, and publish it to dockerhub. Runs only a manual trigger in the github actions tab.","title":"Workflows"},{"location":"developer_guide/workflows/#workflows","text":"","title":"Workflows"},{"location":"developer_guide/workflows/#test","text":"Run the tests on every push/pull request to the _main branch. Writes a coverage report using pytest-cov and uploads it to codecov.io. Tests run against python versions 3.8 and 3.9. Optional manual trigger in the github actions tab.","title":"Test"},{"location":"developer_guide/workflows/#lint","text":"Run the linting tools on every push/pull request to the _main branch. Includes pre-commit hooks, black, isort, flake8, pylint, and mypy. Optional manual trigger in the github actions tab.","title":"Lint"},{"location":"developer_guide/workflows/#release","text":"Build a wheel distribution, build a docker image, create a github release, and publish to PyPI and Docker Hub whenever a new tag is created. Linting and testing steps must pass before the release steps can begin. Documentation is automatically published to the gh-pages branch and hosted on github pages. All github release tags, docker image tags, and PyPI version numbers are in agreement with one another and follow semantic versioning standards.","title":"Release"},{"location":"developer_guide/workflows/#build-and-publish-docs","text":"Build the documentation, publish to the gh-pages branch, and release to github pages. Runs only on a manual trigger in the github actions tab.","title":"Build and Publish Docs"},{"location":"developer_guide/workflows/#build-and-publish-docker-image","text":"Build the docker image, tag it with the branch name, and publish it to dockerhub. Runs only a manual trigger in the github actions tab.","title":"Build and Publish Docker Image"},{"location":"user_guide/collection_scripts/","text":"Gather workload metadata Execute collection script Download the latest collection scripts here . Extract these files to a location that has access to the database from SQL*Plus tar xvf collection_scripts.tar.bz2 cd collector Launch the collection script Execute this from a system that can access your database via sqlplus Pass connect string as input to this script (see below for example) NOTE: If this is an Oracle RAC and/or PDB environment you just need to run it once per database. No need to run in each PDB or in each Oracle RAC instance. ./collect-data.sh \\ optimusprime/Pa55w__rd123@//{db host/scan address}/{service name} Upload Collections Upon completion, the tool will automatically create an archive of the extracted metrics. It is important that this file be submitted as is and without modifications. All files within the archive should follow the following naming convention: opdb__<query_name>__<db_version>_<script_version>_<hostname>_<db_name>_<instance_name>_<datetime>.csv Data is pipe | delimited","title":"Data Collection"},{"location":"user_guide/collection_scripts/#gather-workload-metadata","text":"","title":"Gather workload metadata"},{"location":"user_guide/collection_scripts/#execute-collection-script","text":"Download the latest collection scripts here . Extract these files to a location that has access to the database from SQL*Plus tar xvf collection_scripts.tar.bz2 cd collector Launch the collection script Execute this from a system that can access your database via sqlplus Pass connect string as input to this script (see below for example) NOTE: If this is an Oracle RAC and/or PDB environment you just need to run it once per database. No need to run in each PDB or in each Oracle RAC instance. ./collect-data.sh \\ optimusprime/Pa55w__rd123@//{db host/scan address}/{service name}","title":"Execute collection script"},{"location":"user_guide/collection_scripts/#upload-collections","text":"Upon completion, the tool will automatically create an archive of the extracted metrics. It is important that this file be submitted as is and without modifications. All files within the archive should follow the following naming convention: opdb__<query_name>__<db_version>_<script_version>_<hostname>_<db_name>_<instance_name>_<datetime>.csv Data is pipe | delimited","title":"Upload Collections"},{"location":"user_guide/db_assessment/","text":"Usage Upload & Process Collections Much of the data import and report generation has been automated. Follow section 2.1 to use the automated process. Section 2.2 provides instructions for the manual process if that is your preference. Both processes assume you have rights to create datasets in a Big Query project and access to Data Studio. Make note of the project name and the data set name and location. The data set will be created if it does not exist. Automated load process These instructions are written for running in a Cloud Shell environment. Ensure that your environment is configured to access the Google Cloud project you want to use: gcloud config set project [PROJECT_ID] Create a workspace for processing Create a folder where you upload collections and install the latest collection command line processing utility. export WORKING_DIR=./migration_advisor mkdir -p $WORKING_DIR/data cd $WORKING_DIR if [ ! -f .venv/bin/activate ]; then echo \"Creating new virtual environment.\" python3 -m venv .venv fi source .venv/bin/activate pip install -U wheel setuptools cython pip pip install -U git+https://github.com/GoogleCloudPlatform/oracle-database-assessment.git@main TIP: Google Cloud Shell is a great place to execute these commands. Prepare Collections The utility expects to receive the compressed archives for processing. Ex: mkdir ~/data <upload files to data> cd data <decompress files> Configure automation The automated load process is configured via setting several environment variables and then executing a set of scripts from the base of the git repository. Set these environment variables prior to starting the data load process: # Required # This is the name of the project into which you want to load data export PROJECTNAME=yourProjectNameHere # Required # This is the name of the data set into which you want to load. # The dataset will be created if it does not exist. # If the datset already exists, it will have this data appoended. # Use only alphanumeric characters, - (dash) or _ (underscore) # This name must be filesystem and html compatible export DSNAME=yourDatasetNameHere # Required # This is the location in which the dataset should be created. export DSLOC=yourRegionNameHere # Required # This is the full path into which the customer's files have been extracted. export OPOUTPUTDIR=/full/Path/To/CollectionFiles # Optional # This is the name of the report you want to create in DataStudio upon load completion. # Use only alphanumeric characters or embed HTML encoding. # Defaults to \"OptimusPrime%20Dashboard%20${DSNAME}\" export REPORTNAME=yourReportNameHere # Optional # This is the column separator used in the input data files. # Previous versions of Optimus Prime used ';' (semicolon). # Defaults to | (pipe) export COLSEP='|' Execute the load scripts The load scripts expect to be run from the /oracle-database-assessment/scripts directory. Change to this directory and run the following commands in numeric order. Check output of each for errors before continuing to the next. ./scripts/1_activate_op.sh ./scripts/2_load_op.sh ./scripts/3_run_op_etl.sh ./scripts/4_gen_op_report_url.sh The function of each script is as follows. _configure_op_env.sh - Defines environment variables that are used in the other scripts. This script is executed only by the other scripts in the loading process. 1_activate_op.sh - Installs necessary Python support modules and activates the Python virtual environment for Optimus Prime. 2_load_op.sh - Loads the client data files into the base Optimus Prime tables in the requested data set. 3_run_op_etl.sh - Installs and runs Big Query procedures that create additional views and tables to support the Optimus Prime dashboard. 4_gen_op_report_url.sh - Generates the URL to view the newly loaded data using a report template. Step 3 - Analyzing imported data Clone DataStudio report Click the link displayed by script 4_gen_op_report_url.sh to view the report. Note that this link does not persist the report. To save the report for future use, click the '\"Edit and Share\"' button, then '\"Acknowledge and Save\"', then '\"Add to Report\"'. It will then show up in Data Studio in '\"Reports owned by me\"' and can be shared with others. Skip to step 3 to perform additional analysis for anything not contained in the dashboard report. Open the dataset used in the step 2 of Part 2 in Google BigQuery Query the view names starting with vReport* for further analysis Sample queries are listed, they provide Source DB Summary Source Host details Google Bare Metal Sizing Google Bare Metal Pricing Migration Recommendations","title":"Usage"},{"location":"user_guide/db_assessment/#usage","text":"","title":"Usage"},{"location":"user_guide/db_assessment/#upload-process-collections","text":"Much of the data import and report generation has been automated. Follow section 2.1 to use the automated process. Section 2.2 provides instructions for the manual process if that is your preference. Both processes assume you have rights to create datasets in a Big Query project and access to Data Studio. Make note of the project name and the data set name and location. The data set will be created if it does not exist.","title":"Upload &amp; Process Collections"},{"location":"user_guide/db_assessment/#automated-load-process","text":"These instructions are written for running in a Cloud Shell environment. Ensure that your environment is configured to access the Google Cloud project you want to use: gcloud config set project [PROJECT_ID]","title":"Automated load process"},{"location":"user_guide/db_assessment/#create-a-workspace-for-processing","text":"Create a folder where you upload collections and install the latest collection command line processing utility. export WORKING_DIR=./migration_advisor mkdir -p $WORKING_DIR/data cd $WORKING_DIR if [ ! -f .venv/bin/activate ]; then echo \"Creating new virtual environment.\" python3 -m venv .venv fi source .venv/bin/activate pip install -U wheel setuptools cython pip pip install -U git+https://github.com/GoogleCloudPlatform/oracle-database-assessment.git@main TIP: Google Cloud Shell is a great place to execute these commands.","title":"Create a workspace for processing"},{"location":"user_guide/db_assessment/#prepare-collections","text":"The utility expects to receive the compressed archives for processing. Ex: mkdir ~/data <upload files to data> cd data <decompress files>","title":"Prepare Collections"},{"location":"user_guide/db_assessment/#configure-automation","text":"The automated load process is configured via setting several environment variables and then executing a set of scripts from the base of the git repository. Set these environment variables prior to starting the data load process: # Required # This is the name of the project into which you want to load data export PROJECTNAME=yourProjectNameHere # Required # This is the name of the data set into which you want to load. # The dataset will be created if it does not exist. # If the datset already exists, it will have this data appoended. # Use only alphanumeric characters, - (dash) or _ (underscore) # This name must be filesystem and html compatible export DSNAME=yourDatasetNameHere # Required # This is the location in which the dataset should be created. export DSLOC=yourRegionNameHere # Required # This is the full path into which the customer's files have been extracted. export OPOUTPUTDIR=/full/Path/To/CollectionFiles # Optional # This is the name of the report you want to create in DataStudio upon load completion. # Use only alphanumeric characters or embed HTML encoding. # Defaults to \"OptimusPrime%20Dashboard%20${DSNAME}\" export REPORTNAME=yourReportNameHere # Optional # This is the column separator used in the input data files. # Previous versions of Optimus Prime used ';' (semicolon). # Defaults to | (pipe) export COLSEP='|'","title":"Configure automation"},{"location":"user_guide/db_assessment/#execute-the-load-scripts","text":"The load scripts expect to be run from the /oracle-database-assessment/scripts directory. Change to this directory and run the following commands in numeric order. Check output of each for errors before continuing to the next. ./scripts/1_activate_op.sh ./scripts/2_load_op.sh ./scripts/3_run_op_etl.sh ./scripts/4_gen_op_report_url.sh The function of each script is as follows. _configure_op_env.sh - Defines environment variables that are used in the other scripts. This script is executed only by the other scripts in the loading process. 1_activate_op.sh - Installs necessary Python support modules and activates the Python virtual environment for Optimus Prime. 2_load_op.sh - Loads the client data files into the base Optimus Prime tables in the requested data set. 3_run_op_etl.sh - Installs and runs Big Query procedures that create additional views and tables to support the Optimus Prime dashboard. 4_gen_op_report_url.sh - Generates the URL to view the newly loaded data using a report template.","title":"Execute the load scripts"},{"location":"user_guide/db_assessment/#step-3-analyzing-imported-data","text":"","title":"Step 3 - Analyzing imported data"},{"location":"user_guide/db_assessment/#clone-datastudio-report","text":"Click the link displayed by script 4_gen_op_report_url.sh to view the report. Note that this link does not persist the report. To save the report for future use, click the '\"Edit and Share\"' button, then '\"Acknowledge and Save\"', then '\"Add to Report\"'. It will then show up in Data Studio in '\"Reports owned by me\"' and can be shared with others. Skip to step 3 to perform additional analysis for anything not contained in the dashboard report.","title":"Clone DataStudio report"},{"location":"user_guide/db_assessment/#open-the-dataset-used-in-the-step-2-of-part-2-in-google-bigquery","text":"Query the view names starting with vReport* for further analysis Sample queries are listed, they provide Source DB Summary Source Host details Google Bare Metal Sizing Google Bare Metal Pricing Migration Recommendations","title":"Open the dataset used in the step 2 of Part 2 in Google BigQuery"},{"location":"user_guide/db_user_create/","text":"Database User Scripts (Optional) The collection scripts can be executed with any DBA account. Alternately, a new user with the minimum privileges required for access with the following steps. Create User PDB (Recommended) create user optimusprime identified by \"Pa55w__rd123\"; CDB select * from v$system_parameter where name='common_user_prefix'; --C## create user C##optimusprime identified by \"Pa55w__rd123\"; Grants From the directory you extracted the collector scripts: @sql/grants_wrapper.sql -- It will prompt for the user created above NOTE: grants_wrapper.sql has provided variable db_awr_license which is set default to Y to access AWR tables. AWR is a licensed feature of Oracle. If you don't have license to run AWR you can disable flag and it will execute script minimum_select_grants_for_targets_ONLY_FOR_11g.sql.","title":"Database User Scripts"},{"location":"user_guide/db_user_create/#database-user-scripts-optional","text":"The collection scripts can be executed with any DBA account. Alternately, a new user with the minimum privileges required for access with the following steps.","title":"Database User Scripts (Optional)"},{"location":"user_guide/db_user_create/#create-user","text":"","title":"Create User"},{"location":"user_guide/db_user_create/#pdb-recommended","text":"create user optimusprime identified by \"Pa55w__rd123\";","title":"PDB (Recommended)"},{"location":"user_guide/db_user_create/#cdb","text":"select * from v$system_parameter where name='common_user_prefix'; --C## create user C##optimusprime identified by \"Pa55w__rd123\";","title":"CDB"},{"location":"user_guide/db_user_create/#grants","text":"From the directory you extracted the collector scripts: @sql/grants_wrapper.sql -- It will prompt for the user created above NOTE: grants_wrapper.sql has provided variable db_awr_license which is set default to Y to access AWR tables. AWR is a licensed feature of Oracle. If you don't have license to run AWR you can disable flag and it will execute script minimum_select_grants_for_targets_ONLY_FOR_11g.sql.","title":"Grants"},{"location":"user_guide/installation/","text":"Usage Importing the data collected into Google BigQuery for analysis Much of the data import and report generation has been automated. Follow section 2.1 to use the automated process. Section 2.2 provides instructions for the manual process if that is your preference. Both processes assume you have rights to create datasets in a Big Query project and access to Data Studio. Make note of the project name and the data set name and location. The data set will be created if it does not exist. Automated load process These instructions are written for running in a Cloud Shell environment. Ensure that your environment is configured to access the Google Cloud project you want to use: gcloud config set project [PROJECT_ID] Create a workspace for processing Create a folder where you upload collections and install the latest collection command line processing utility. export WORKING_DIR=./migration_advisor mkdir -p $WORKING_DIR/data cd $WORKING_DIR if [ ! -f .venv/bin/activate ]; then echo \"Creating new virtual environment.\" python3 -m venv .venv fi source .venv/bin/activate pip install -U wheel setuptools cython pip pip install -U git+https://github.com/GoogleCloudPlatform/oracle-database-assessment.git@main TIP: Google Cloud Shell is a great place to execute these commands.","title":"Usage"},{"location":"user_guide/installation/#usage","text":"","title":"Usage"},{"location":"user_guide/installation/#importing-the-data-collected-into-google-bigquery-for-analysis","text":"Much of the data import and report generation has been automated. Follow section 2.1 to use the automated process. Section 2.2 provides instructions for the manual process if that is your preference. Both processes assume you have rights to create datasets in a Big Query project and access to Data Studio. Make note of the project name and the data set name and location. The data set will be created if it does not exist.","title":"Importing the data collected into Google BigQuery for analysis"},{"location":"user_guide/installation/#automated-load-process","text":"These instructions are written for running in a Cloud Shell environment. Ensure that your environment is configured to access the Google Cloud project you want to use: gcloud config set project [PROJECT_ID]","title":"Automated load process"},{"location":"user_guide/installation/#create-a-workspace-for-processing","text":"Create a folder where you upload collections and install the latest collection command line processing utility. export WORKING_DIR=./migration_advisor mkdir -p $WORKING_DIR/data cd $WORKING_DIR if [ ! -f .venv/bin/activate ]; then echo \"Creating new virtual environment.\" python3 -m venv .venv fi source .venv/bin/activate pip install -U wheel setuptools cython pip pip install -U git+https://github.com/GoogleCloudPlatform/oracle-database-assessment.git@main TIP: Google Cloud Shell is a great place to execute these commands.","title":"Create a workspace for processing"},{"location":"user_guide/manual_load_process/","text":"Manual load process Setup Environment variables (From Google Cloud Shell ONLY) gcloud auth list gcloud config set project <project id> Export Environment variables. (Step 1.2 has working directory created) export OP_WORKING_DIR=$(pwd) export OP_BQ_DATASET=[Dataset Name] export OP_OUTPUT_DIR=/$OP_WORKING_DIR/oracle-database-assessment-output/<assessment output directory> mkdir $OP_OUTPUT_DIR/log export OP_LOG_DIR=$OP_OUTPUT_DIR/log Create working directory (Skip if you have followed step 1.2 on same server) mkdir $OP_WORKING_DIR Clone Github repository (Skip if you have followed step 1.2 on same server) cd <work-directory> git clone https://github.com/GoogleCloudPlatform/oracle-database-assessment Create assessment output directory mkdir -p /<work-directory>/oracle-database-assessment-output cd /<work-directory>/oracle-database-assessment-output Move zip files to assessment output directory and unzip mv <file file> /<work-directory>/oracle-database-assessment-output unzip <zip files> Create a service account and download the key Set GOOGLE_APPLICATION_CREDENTIALS to point to the downloaded key. Make sure the service account has BigQuery Admin privilege. NOTE: This step can be skipped if using Cloud Shell Create a python virtual environment to install dependencies and execute the optimusprime.py script python3 -m venv $OP_WORKING_DIR/.venv source $OP_WORKING_DIR/.venv/bin/activate cd $OP_WORKING_DIR/oracle-database-assessment/ pip3 install pip wheel setuptools --upgrade pip3 install . # If you want to import one single Optimus Prime file collection (From 1 single database), please follow the below step: optimus-prime -dataset newdatasetORexistingdataset -collectionid 080421224807 --files-location /<work-directory>/oracle-database-assessment-output --project-name my-awesome-gcp-project -importcomment \"this is for prod\" # If you want to import various Optimus Prime file collections (From various databases) that are stored under the same directory being used for --files-location. Then, you can add to your command two additional flags (--from-dataframe -consolidatedataframes) and pass only \"\" to -collectionid. See example below: optimus-prime -dataset newdatasetORexistingdataset -collectionid \"\" --files-location /<work-directory>/oracle-database-assessment-output --project-name my-awesome-gcp-project --from-dataframe -consolidatedataframes # If you want to import only specific db version or sql version from Optimus Prime file collections hat are stored under the same directory being used for --files-location. optimus-prime -dataset newdatasetORexistingdataset -collectionid \"\" --files-location /<work-directory>/oracle-database-assessment-output --project-name my-awesome-gcp-project --from-dataframe -consolidatedataframes --filter-by-db-version 11.1 --filter-by-sql-version 2.0.3 # If you want to akip all file validations optimus-prime -dataset newdatasetORexistingdataset -collectionid \"\" --files-location /<work-directory>/oracle-database-assessment-output --project-name my-awesome-gcp-project -skipvalidations --dataset : is the name of the dataset in Google BigQuery. It is created if it does not exists. If it does already nothing to do then. --collection-id : is the file identification which last numbers in the filename which represents <datetime> (mmddrrhh24miss) . In this example of a filename opdb__usedspacedetails__121_0.1.0_mydbhost.mycompany.com.ORCLDB.orcl1.071621111714.log the file identification is 071621111714 . --files-location : The location in which the opdb*log were saved. --project-name : The GCP project in which the data will be loaded. --delete-dataset : This an optional. In case you want to delete the whole existing dataset before importing the data. WARNING: It will DELETE permanently ALL tables previously in the dataset. No further confirmation will be required. Use it with caution. --import-comment : This an optional. In case you want to store any comment about the load in opkeylog table. Eg: \"This is for Production import\" --filter-by-sql-version : This an optional. In case you have files from multiple sql versions in the folder and you want to load only specific sql version files --filter-by-db-version : This an optional. In case you have files from multiple db versions in the folder and you want to load only specific db version files --skip-validations : This is optional. Default is False. if we use the flag, file validations will be skipped NOTE: If your file has elapsed time or any other string except data, run the following script to remove it for i in `grep \"Elapsed:\" $OP_OUTPUT_DIR/*.log | cut -d \":\" -f 1`; do sed -i '$ d' $i; done","title":"Manual load process"},{"location":"user_guide/manual_load_process/#manual-load-process","text":"","title":"Manual load process"},{"location":"user_guide/manual_load_process/#setup-environment-variables-from-google-cloud-shell-only","text":"gcloud auth list gcloud config set project <project id>","title":"Setup Environment variables (From Google Cloud Shell ONLY)"},{"location":"user_guide/manual_load_process/#export-environment-variables-step-12-has-working-directory-created","text":"export OP_WORKING_DIR=$(pwd) export OP_BQ_DATASET=[Dataset Name] export OP_OUTPUT_DIR=/$OP_WORKING_DIR/oracle-database-assessment-output/<assessment output directory> mkdir $OP_OUTPUT_DIR/log export OP_LOG_DIR=$OP_OUTPUT_DIR/log","title":"Export Environment variables. (Step 1.2 has working directory created)"},{"location":"user_guide/manual_load_process/#create-working-directory-skip-if-you-have-followed-step-12-on-same-server","text":"mkdir $OP_WORKING_DIR","title":"Create working directory (Skip if you have followed step 1.2 on same server)"},{"location":"user_guide/manual_load_process/#clone-github-repository-skip-if-you-have-followed-step-12-on-same-server","text":"cd <work-directory> git clone https://github.com/GoogleCloudPlatform/oracle-database-assessment","title":"Clone Github repository (Skip if you have followed step 1.2 on same server)"},{"location":"user_guide/manual_load_process/#create-assessment-output-directory","text":"mkdir -p /<work-directory>/oracle-database-assessment-output cd /<work-directory>/oracle-database-assessment-output","title":"Create assessment output directory"},{"location":"user_guide/manual_load_process/#move-zip-files-to-assessment-output-directory-and-unzip","text":"mv <file file> /<work-directory>/oracle-database-assessment-output unzip <zip files>","title":"Move zip files to assessment output directory and unzip"},{"location":"user_guide/manual_load_process/#create-a-service-account-and-download-the-key","text":"Set GOOGLE_APPLICATION_CREDENTIALS to point to the downloaded key. Make sure the service account has BigQuery Admin privilege. NOTE: This step can be skipped if using Cloud Shell","title":"Create a service account and download the key"},{"location":"user_guide/manual_load_process/#create-a-python-virtual-environment-to-install-dependencies-and-execute-the-optimusprimepy-script","text":"python3 -m venv $OP_WORKING_DIR/.venv source $OP_WORKING_DIR/.venv/bin/activate cd $OP_WORKING_DIR/oracle-database-assessment/ pip3 install pip wheel setuptools --upgrade pip3 install . # If you want to import one single Optimus Prime file collection (From 1 single database), please follow the below step: optimus-prime -dataset newdatasetORexistingdataset -collectionid 080421224807 --files-location /<work-directory>/oracle-database-assessment-output --project-name my-awesome-gcp-project -importcomment \"this is for prod\" # If you want to import various Optimus Prime file collections (From various databases) that are stored under the same directory being used for --files-location. Then, you can add to your command two additional flags (--from-dataframe -consolidatedataframes) and pass only \"\" to -collectionid. See example below: optimus-prime -dataset newdatasetORexistingdataset -collectionid \"\" --files-location /<work-directory>/oracle-database-assessment-output --project-name my-awesome-gcp-project --from-dataframe -consolidatedataframes # If you want to import only specific db version or sql version from Optimus Prime file collections hat are stored under the same directory being used for --files-location. optimus-prime -dataset newdatasetORexistingdataset -collectionid \"\" --files-location /<work-directory>/oracle-database-assessment-output --project-name my-awesome-gcp-project --from-dataframe -consolidatedataframes --filter-by-db-version 11.1 --filter-by-sql-version 2.0.3 # If you want to akip all file validations optimus-prime -dataset newdatasetORexistingdataset -collectionid \"\" --files-location /<work-directory>/oracle-database-assessment-output --project-name my-awesome-gcp-project -skipvalidations --dataset : is the name of the dataset in Google BigQuery. It is created if it does not exists. If it does already nothing to do then. --collection-id : is the file identification which last numbers in the filename which represents <datetime> (mmddrrhh24miss) . In this example of a filename opdb__usedspacedetails__121_0.1.0_mydbhost.mycompany.com.ORCLDB.orcl1.071621111714.log the file identification is 071621111714 . --files-location : The location in which the opdb*log were saved. --project-name : The GCP project in which the data will be loaded. --delete-dataset : This an optional. In case you want to delete the whole existing dataset before importing the data. WARNING: It will DELETE permanently ALL tables previously in the dataset. No further confirmation will be required. Use it with caution. --import-comment : This an optional. In case you want to store any comment about the load in opkeylog table. Eg: \"This is for Production import\" --filter-by-sql-version : This an optional. In case you have files from multiple sql versions in the folder and you want to load only specific sql version files --filter-by-db-version : This an optional. In case you have files from multiple db versions in the folder and you want to load only specific db version files --skip-validations : This is optional. Default is False. if we use the flag, file validations will be skipped NOTE: If your file has elapsed time or any other string except data, run the following script to remove it for i in `grep \"Elapsed:\" $OP_OUTPUT_DIR/*.log | cut -d \":\" -f 1`; do sed -i '$ d' $i; done","title":"Create a python virtual environment to install dependencies and execute the optimusprime.py script"},{"location":"user_guide/permissions/","text":"Create a user for Collection The collection scripts can be executed with any DBA account. Alternately, a new user with the minimum privileges required for access with the following steps. Permissions Required Views: * AUX_STATS$ * CDB_CONSTRAINTS * CDB_DATA_FILES * CDB_DB_LINKS * CDB_EXTERNAL_TABLES * CDB_FEATURE_USAGE_STATISTICS * CDB_FREE_SPACE * CDB_HIGH_WATER_MARK_STATISTICS * CDB_HIST_ACTIVE_SESS_HISTORY * CDB_HIST_IOSTAT_FUNCTION * CDB_HIST_OSSTAT * CDB_HIST_SNAPSHOT * CDB_HIST_SQLSTAT * CDB_HIST_SQLTEXT * CDB_HIST_SYSMETRIC_HISTORY * CDB_HIST_SYSMETRIC_SUMMARY * CDB_HIST_SYSSTAT * CDB_HIST_SYSTEM_EVENT * CDB_HIST_SYS_TIME_MODEL * CDB_INDEXES * CDB_OBJECTS * CDB_PART_TABLES * CDB_PDBS * CDB_SEGMENTS * CDB_SERVICES * CDB_SOURCE * CDB_TAB_COLUMNS * CDB_TABLES * CDB_TABLESPACES * CDB_TAB_PARTITIONS * CDB_TAB_SUBPARTITIONS * CDB_USERS * DBA_CONSTRAINTS * DBA_CPU_USAGE_STATISTICS * DBA_DATA_FILES * DBA_DB_LINKS * DBA_EXTERNAL_TABLES * DBA_FEATURE_USAGE_STATISTICS * DBA_FREE_SPACE * DBA_HIGH_WATER_MARK_STATISTICS * DBA_HIST_ACTIVE_SESS_HISTORY * DBA_HIST_IOSTAT_FUNCTION * DBA_HIST_OSSTAT * DBA_HIST_SNAPSHOT * DBA_HIST_SQLSTAT * DBA_HIST_SQLTEXT * DBA_HIST_SYSMETRIC_HISTORY * DBA_HIST_SYSMETRIC_SUMMARY * DBA_HIST_SYSSTAT * DBA_HIST_SYSTEM_EVENT * DBA_HIST_SYS_TIME_MODEL * DBA_INDEXES * DBA_OBJECTS * DBA_PART_TABLES * DBA_REGISTRY_SQLPATCH * DBA_SEGMENTS * DBA_SERVICES * DBA_SOURCE * DBA_TAB_COLUMNS * DBA_TABLES * DBA_TABLESPACES * DBA_TAB_PARTITIONS * DBA_TAB_SUBPARTITIONS * DBA_USERS * GV_$ARCHIVE_DEST * GV_$ARCHIVED_LOG * GV_$INSTANCE * GV_$PARAMETER * LOGSTDBY$SKIP_SUPPORT * NLS_DATABASE_PARAMETERS * REGISTRY$HISTORY * V_$DATABASE * V_$DIAG_ALERT_EXT * V_$INSTANCE * V_$LOG * V_$LOG_HISTORY * V_$PDBS * V_$PGASTAT * V_$RMAN_BACKUP_JOB_DETAILS * V_$SGASTAT * V_$SQLCOMMAND * V_$TEMP_SPACE_HEADER * V_$VERSION","title":"Permissions Required"},{"location":"user_guide/permissions/#create-a-user-for-collection","text":"The collection scripts can be executed with any DBA account. Alternately, a new user with the minimum privileges required for access with the following steps.","title":"Create a user for Collection"},{"location":"user_guide/permissions/#permissions-required","text":"Views: * AUX_STATS$ * CDB_CONSTRAINTS * CDB_DATA_FILES * CDB_DB_LINKS * CDB_EXTERNAL_TABLES * CDB_FEATURE_USAGE_STATISTICS * CDB_FREE_SPACE * CDB_HIGH_WATER_MARK_STATISTICS * CDB_HIST_ACTIVE_SESS_HISTORY * CDB_HIST_IOSTAT_FUNCTION * CDB_HIST_OSSTAT * CDB_HIST_SNAPSHOT * CDB_HIST_SQLSTAT * CDB_HIST_SQLTEXT * CDB_HIST_SYSMETRIC_HISTORY * CDB_HIST_SYSMETRIC_SUMMARY * CDB_HIST_SYSSTAT * CDB_HIST_SYSTEM_EVENT * CDB_HIST_SYS_TIME_MODEL * CDB_INDEXES * CDB_OBJECTS * CDB_PART_TABLES * CDB_PDBS * CDB_SEGMENTS * CDB_SERVICES * CDB_SOURCE * CDB_TAB_COLUMNS * CDB_TABLES * CDB_TABLESPACES * CDB_TAB_PARTITIONS * CDB_TAB_SUBPARTITIONS * CDB_USERS * DBA_CONSTRAINTS * DBA_CPU_USAGE_STATISTICS * DBA_DATA_FILES * DBA_DB_LINKS * DBA_EXTERNAL_TABLES * DBA_FEATURE_USAGE_STATISTICS * DBA_FREE_SPACE * DBA_HIGH_WATER_MARK_STATISTICS * DBA_HIST_ACTIVE_SESS_HISTORY * DBA_HIST_IOSTAT_FUNCTION * DBA_HIST_OSSTAT * DBA_HIST_SNAPSHOT * DBA_HIST_SQLSTAT * DBA_HIST_SQLTEXT * DBA_HIST_SYSMETRIC_HISTORY * DBA_HIST_SYSMETRIC_SUMMARY * DBA_HIST_SYSSTAT * DBA_HIST_SYSTEM_EVENT * DBA_HIST_SYS_TIME_MODEL * DBA_INDEXES * DBA_OBJECTS * DBA_PART_TABLES * DBA_REGISTRY_SQLPATCH * DBA_SEGMENTS * DBA_SERVICES * DBA_SOURCE * DBA_TAB_COLUMNS * DBA_TABLES * DBA_TABLESPACES * DBA_TAB_PARTITIONS * DBA_TAB_SUBPARTITIONS * DBA_USERS * GV_$ARCHIVE_DEST * GV_$ARCHIVED_LOG * GV_$INSTANCE * GV_$PARAMETER * LOGSTDBY$SKIP_SUPPORT * NLS_DATABASE_PARAMETERS * REGISTRY$HISTORY * V_$DATABASE * V_$DIAG_ALERT_EXT * V_$INSTANCE * V_$LOG * V_$LOG_HISTORY * V_$PDBS * V_$PGASTAT * V_$RMAN_BACKUP_JOB_DETAILS * V_$SGASTAT * V_$SQLCOMMAND * V_$TEMP_SPACE_HEADER * V_$VERSION","title":"Permissions Required"}]}